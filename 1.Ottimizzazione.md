# Ottimizzazione

> Compile time è il momento in cui sto compilando il programma. Runtime è il momento in cui sto eseguendo il programma.

## Ottimizzazioni algebriche

Una doppia negazione si può elidere, oppure un cortocircuito logico.

## Constant folding

Se a compile time ho tutte le informazioni necessarie per eseguire ed estendere espressioni costanti a tempo di
compilazione le valuterò a tempo di compilazione e inserirò il valore inline.

## Strenght reduction

Sostituisce operazioni costose con altre più semplici, tipo rimpiazzare una moltiplicazione con una add/shift.

```
y = x * 2       -> y = x + x
```

oppure logiche più complesse come

```
y = x * 17      -> y = (x << 4) + 4
```

e ho risparmiato un ciclo supponendo che la MUL richieda 3 cicli.

La strenght reduction si può applicare solo in certi casi e non è detto che sia vantaggioso.

Per i cicli che usano un indice per accedere a un array, si può fare LIVE (loop induction variable elimination).
Questo perché in C l'accesso a un array richiede il calcolo dell'offset in bytes.

## Common Subexpression Elimination (CSE)

Elimina i calcoli ridondanti di una stessa espressione usata in più istruzioni. Le ottimizzazione avvengono tipicamente
in cascata, quindi come una catena di effetti e non come un fenomeno isolato.

Può essere propedeutica alla DCE.

## Dead code elimination (DCE)

Rimuove il codice non necessario, ovvero risultati che non vengono mai utilizzati perché inutili al fine dell'esecuzione
del programma.

Variabili mai lette (usate) oppure codice irraggiungibile.

```
b = 3
c = 1 + d
d = 3 + c
```

oppure codice irraggiungibile come:

```
if (100<0)
    { a = 5; }
```

Posso avere, come in questo caso, delle ulteriori ottimizzazioni come una Algebraic Optimization.

## Copy propagation

Per uno statement del tipo `x=y`, sostituisci gli usi futuri di x con y se non sono cambiati nel frattempo.

Anche questo è propedeutico al DCE.

## Constant propagation

Simile alla copy propagation ma andiamo a propagare le costanti. `b=3` sostituisco gli usi futuri di b con la costante
se b non è cambiato nel frattempo.

```
b = 3
c = 1 + b
d = 3 + c
```

faccio constant propagation e folding per avere

```
b = 3
c = 4 (1 + 3)
d = 3 + c
```

posso di nuobo fare constant propagation e folding per avere `d=7`.

Ipotizziamo che solo l'ultimo degli statement abbia un'utilità, con dead code elimination rimaniamo solo con `d=7`.

## LICM Loop Invariant Code Motion

Sposta le istruzioni indipendenti dal loop fuori, detto anche code hoisting. Ottimizzare i loop è importante
perché all'interno di un loop si svolge la maggior parte del lavoro.

Loop invariant vuol dire che le istruzioni non dipendono in alcuna maniera a ciò che avviene all'interno del loop.
Evita i calcoli ridondanti.

Sono particolarmente rilevanti perché sono propedeutiche a ottimizzazioni _machine-specific_ che sono effettuate nel
backend perché sono tutti relativi all'ISA.

- Register allocation (quali registri usare e come, in ottemperanza alla calling convention). I registri sono pochi e
devo continuare spostare la memoria. L'operazione di trascrivere il contenuto di un registro nella memoria è detto
_spilling_ dei registri in memoria (che comporta l'esecuzione di store e load).

    Vogliamo ridurre al minimo il numero di spills perché coinvolgono un accesso in memoria, sebbene attraverso la
    gerarchia di cache.

- Instruction-level parallelism: forma di parallelismo all'interno di uno stesso **thread** di esecuzione, ovvero abbiamo
più istruzioni che eseguano contemporanemanete. L'esempio più semplice è che il progetto della mia CPU è pipelined,
quindi devo organizzare le istruzioni per evitare stalli nella pipeline (evitare i data hazards).

    ```
    add s0, s1, s2
    sub s5, s0, s2
    ```

    Ho un data hazard, solo in fase di writeback avrò il dato disponibile per la `sub`. Se la CPU non ha hardware dedicato
    per il forwarding e l'hazard detection devo inserire delle nop.

    Se inserisco delle bubble riduco il CPI medio.

    Solo nel backend ho a che fare con il progetto della CPU e posso fare _instruction scheduling_. Il VLIW ricade in
    questa tipologia.

- Data parallelism (multi-core, SIMD)

- Data-cache locality. Le cache sono il blocco architetturale più importante per nascondere l'alta latenza della DRAM.
Ma funzionano bene fintanto che il programma ha buona località spaziale e temporale.

    I compilatori riescono a ottimizzare bene per le cache, conoscendo il tipo e la grandezza.

## Cache miss minimization

Per ridurre l'execution time si può ottimizzare, oltre alle istruzioni sul processore, anche il percorso tra processore
e memoria.

In una architettura RISC e in particolare RISC-V, le uniche operazioni che fanno accesso alla memoria e quindi "lavorano"
sul percorso tra processore e memoria sono `LD` e `SD`.

Le performance di un sistema non è tradotta automaticamente nella performance dei programmi. Il compilatore può aiutarci
a migliorare la performance su sistemi multicore con esecuzione parallela più thread.
