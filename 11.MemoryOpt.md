# Ottimizzazioni memoria

C'è un memory wall: DRAM e CPU sono basate su tecnologie diverse, quindi hanno performance differenti. Questo crea una
forbice di performance che cresce tanto più va avanti il tempo.

Per questo motivo nascono gerarchie di cache sempre più complesse, per nascondere questa latenza.

Le DRAM sono memorie off-chip, ovvero tipicamente si trova fuori da un circuito integrato. La motherboard contiene
tipicamente un socket per il chip della CPU e degli slot per alloggiare degli slot.
La comunicazione avviene tra canali che sono esterni alla CPU: uno dei principali motivi per cui la DRAM è lenta.

Inoltre a differenza della CPU (che è una rete logica stampata su un pezzo di silicio e pilotata da un singolo segnale di
clock), mentre la RAM si basa sulla capacità di un circuito (capacità come quello di un condensatore). Il fatto che ci sia
carica rappresenta un 1 logico, se la carica non è presente abbiamo uno 0.

La carica è volatile, perché nel tempo tendono a perdere la loro carica. La RAM viene quindi costantemente riscritta.

> La banda è la quantità di dati che posso trasportare in una singola transazione.

Nelle GPU le interfacce di memoria sono molto large (256/512b) per tenere occupati tutti i processori.

Siccome una transazione ha un costo, leggo una riga intera. Questo è giustificato da località spaziale dei dati che
manipolo. Se i dati che uso spesso vengono contenuti nella stessa riga, allora l'accesso sarà più veloce.

## SRAM

Ci sono altri tipi di tecnologie rispetto alla DRAM.

La SRAM ha la caratteristica di poter essere integrata dentro a un chip, e si utilizza principalmente per realizzare le
gerarchie di cache. Accedere a una SRAM è tipicamente comparabile alla velocità della CPU. Questo dipende anche dalla
distanza fisica tra cache e CPU.

La gerarchia di una cache e dei core è fisicamente distribuita, quindi è "vicina" a tutti i core. Colpisce che una porzione
significativa del SoC è occupata dalla cache. Parliamo di diversi MiB di capacità.

Cosa succederebbe se dovessimo implementare una quantità di memoria come la DRAM? Avremmo bisogno di un die con un'area
molto grande, nonché avremmo un consumo di memoria superiore. La DRAM è più piccola.

## Località

Se accedo a una locazione di memoria, probabilmente accederò anche a quelle contigue. La maggior parte dei programmi
spende tempo nei loop, che manipolano strutture dati tipicamente ospitate in memoria come celle adiacenti.

> Se questo è il modo più comune di usare gli array, progettiamo le cache in modo da portarmi dentro anche i dati
successivi. Questo prende nome di località spaziale.

Empiricamente si osserva questa tendenza di **località spaziale**.

> Se accedo a un elemento (indirizzo di memoria), spesso viene acceduto nuovamente prima o poi.

Questa è **località temporale**, e il prima o poi viene definito dalla struttura della cache.

Nel 1971 il memory gap inizia a essere rilevante, e gli ingegneri di IBM si pongono il problema. Pubblicano il paper
_Program Restructuing for Virtual Memory_. Siamo agli albori della cache.

Si analizza il trace degli accessi alla memoria. Ci sono zone senza località, e zone con accesso ripetuto allo stesso
indirizzo di memoria. Siamo in presenza del fenomeno di temporal locality. (Rette orizzontali)

In presenza di una retta diagonale, siamo in presenza di un loop che legge un array. Due pattern facilmente
identificabili e talmente ricorrenti che giustificano la nascita di un nuovo pezzo di hardware.

## Funzionamento

Il caching avviene anche tra disco e DRAM, ed è quello che fa la memoria paginata.

La cache si interpone tra CPU e DRAM e fa lo _snooping_ degli indirizzi. Ascolta l'indirizzo e copia in maniera preventiva
(oppure reattiva se faccio una miss), e mi porto dentro tutta una linea di cache.
Le linee sfruttano il principio di località spaziale, un indirizzo identifica n-word.
Il principio di località temporale è soddisfatto dalla cache stessa perché mantengo i dati altamente acceduti vicino a me.

### Tipi di miss

Le tre C:

- compulsory (cold cache miss), quando il dato non è presente nella cache;

- conflict, quando uso cache direct mapped o associative ma non completamente. Siccome la cache è una frazione della DRAM
molti indirizzi vanno rimappati sulla stessa linea e questo origina una conflict miss. C'è sempre un tradeoff, tanto che
le cache più utilizzate sono ibride, e vengono dette set associative.

- capacity, quando non c'è più spazio libero nella cache.

## Exploiting locality

Per sfruttare la località spaziale devo ammortizzare il costo di una miss, quindi mi porto dentro tutta una linea di cache.
Più è lungo un blocco più riesco a ridurre la miss rate per località spaziale.

La dimensione della cache è fissa, e linee più grandi potrebbero darmi anche più eviction dovute a una pollution. Questo è
dovuto alla competizione tra linee di cache, che dovrei poter sostituire continuamente.

Dentro a un compilatore riesco a ragionare sul capire come sfruttare al meglio la gerarchia di cache. I programmi sono tutti
diversi e a livello architetturale non si può risolvere il problema, quindi lo faccio nel compilatore che è il punto migliore.

## Obiettivi

Voglio **ridurre la latenza**, quindi fare in modo che la maggior parte degli accessi in memoria risultano come hits.

Per **massimizzare la larghezza di banda** devo sfruttare le letture burst della RAM, ovvero letture in maxi-transazioni
di linee continue nella DRAM. La granularità diaccesso alla DRAM è la cache.

## Oggetti

In un programma manipoliamo tre tipi di dato.

- Scalari: tipi di dato semplici, come variabili statiche globali, parametri a funzione.

- Strutture dati: la clasica struct del C che organizza i dati aggregandoli.

- Array.

### Scalari

Quando uso variabili scalari, spesso si traduce in registri. Quindi se lavoro a livello di registri, la cache non
interviene affatto. Quindi a livello di local variable non ho gran problemi.

In assenza di informazioni su come è fatto un vero register file, si può assumere che ci siano illimitati registri
virtuali quando si ottimizza il codice intermedio e quindi si usa `mem2reg`. Oppure si assume che non ci siano registri
quindi si usano una sfilza di L/S.

Nel backend poi si fa il mapping sui registri fisici. Fin tanto che ho registri li uso, ma dopo devo usare lo stack per
le variabili locali (principalmente lo usavamo per la gestione della calling convention, ma anche gli argomenti che non riesco
a gestire direttamente tramite registro vanno passati con lo stack).

Passi come `mem2reg` o il suo duale servono a ottimizzare questo. Register allocation (viene fatta nel backend) servono
proprio a gestire questo genere di problematiche.

Quindi anche nell'uso di scalari il problema c'è e i compilatori possono gestirlo.

### Strutture dati e puntatori

In una situazione agnostica, senza padding e allineamento alla machine word, un campo potrebbe finire tra due righe
di DRAM. Questo mi costringe a caricare due righe.

Dove non riesco ad allinearmi semplicemente cambiando posto ai campi, posso fare padding della struttura dati in modo
che siano allineati.

Per ottimizzare posso trasformare la struttura, spezzandola.

## Ottimizzazioni array

Strutture molto regolari, anche multidimensionali e facili da ottimizzare a compile time; quindi a seguito di un'analisi
statica.

Per rappresentare gli accessi in memoria uso un poliedro, chiamato iteration space che definisce l'ordine di visita degli
elementi dell'array.

Abbiamo anche il data space che considera, presa una cache con una dimensione, quali locazioni producono hit e miss.

Se il numero di righe o colonne è maggiore delle linee di cache, sto creando un conflitto perché mappo sulla stessa linea
un indirizzo diverso. Quindi questo algoritmo naive di trasposizione genera solo delle miss.

### Locality analysis

Analisi statica per modificare le iterazioni dell'array o il layout dei dati per ottimizzare. Per gli interi non ci sono
mai problemi, per operazioni in aritmetica finita o floating point anche l'ordine è importante.

Approximate computing: si sfrutta l'idea che le applicazioni siano tolleranti, quindi riscrivo l'applicazione in modo
che possa essere riorganizzata tenendo sotto controllo la variabilità tra errore e soluzione.

La **data dependence analysis** permette di capire se il programma è formalmente corretto dopo aver applicato le
trasformazioni.

## Tecniche di ottimizzazione

La tecnica più semplice è quella di loop interchange: se l'algoritmo è row-major, si scambia l'ordine di attraversamento
per lavorare in modo più fine sulle righe, che sono mappate nella cache. Si chiama anche loop queueing o permutation.

È propedeutico anche ad altre analisi. Più mi sposto all'interno di un loop più sfrutterò il cosiddetto parallelismo a
grana fine, mentre spostandomi all'esterno ho parallelismo a grana grossa.

Le GPU sono pesantemente parallele a grado finissimo, infatti dividono i dati da processare su tutti i processi.

### Coarse-grained parallelism

Se invece sto cercando di usare parallelismo a grana grossa, qullo che succede a livello di thread.
La cosa più adatta è quindi tagliare "per righe" che vengono assegnate ai vari processori. Per identificare questa
forma di parallelismo si lavora a livello più esterno.

Bisogna anche vedere qual è l'overhead di avviare un nuovo thread.

### Matrix multiplication

Calcolare una cella del risultato prevede di accedere a un'intera riga di A, che potrebbe non stare in cache, e a un'intera
colonna di B che potrebbe avere una località spaziale povera.

Da qui l'idea di tiling/blocking, altero la struttura dati per sfruttare tutto il riuso: prima su porzioni che stanno
all'interno della cache.

Supponiamo che in cache ci stiano N righe di M byte, allora processo la struttura come se fosssero blocchi N*M.
Alteriamo l'iteration space per catturare la località del programma e massimizzare l'uso della cache.

Dal punto di vista del codice il tiling inserisce livelli extra del loop (in una matrix multiplication ne inserisce 2)
e adesso processo N blocchi dove N è la lunghezza di una riga di A diviso per la riga di cache.
Si ottengono due ordini di grandezza nel fare questa ottimizzazione.

In CUDA le cache sono esplicitamente gestite, a differenza di ciò che stiamo facendo qua dove le cache sono trasparenti.

## Loop fusion

Loop con iteration space della stessa dimensione e pattern di accesso, nonché lo stesso stride (incremento sulle variabili)
possono essere fusi.

Il programma esibisce la località temporale perché sto riusando la matrice `a`.

```c
for (i=0; i<N; i++)
    for (j=0; j<N; j++)
        a[i][j] = 1 / b[i][j] * c[i][j]

for (i=0; i<N; i++)
    for (j=0; j<N; j++)
        d[i][j] = a[i][j] + c[i][j]
```

Sfrutta la località temporale? Ovvero nel secondo loop non ho cache miss quando accedo ad A?
L'affermazione è vera solo se tutta la matrice A può essere contenuta nella cache, ovvero la sua memory footprint è
inferiore alla dimensione della cache.

In questa formulazione, se A è maggiore della dimensione della cache, alcuni accessi in memoria comportanto delle eviction
causate da un cache miss.

Il secondo loop ricomincia quindi "da capo" ad accedere alla memoria, quindi non cattura la località temporale.

È un caso rarissimo che le matrici stiano all'interno della cache. In che modo la loop fusion può aiutarci a sfruttare la
località temporale?

Se riscrivo il problema così, ho la garanzia di aver avvicinato quando scrivo il valore in A e quando lo uso. Far passare
tanto tempo potrebbe significare un'eviction della cache.

```c
for (i=0; i<N; i++)
    for (j=0; j<N; j++)
        a[i][j] = 1 / b[i][j] * c[i][j]
        d[i][j] = a[i][j] + c[i][j]
```

Anticipare l'uso del dato garantisce che esso sarà ancora in cache quando lo dovrò usare.

Prima di ottimizzare devo capire la footprint della struttura in cache. Analizzo gli indici:

- inizio, fine, incremento. La footprint equivale a $(fine - inizio) / incremento$.

## Software prefetching

> L'idea di fondo è che se mi porto in anticipo in cache dei dati, essi saranno presenti non appena mi vengono richiesti.

Quindi ammortizzo la latenza di caricare i dati.

Nel codice posso caricare delle operazioni prefetching esplicito, spostando il dato "vicino" al processore prima del
momento in cui sarà necessario averlo.

Lo scopo delle operazioni di prefetch è quello di sfruttare operazioni per triggerare un fetch anticipato nella gerarchia
di cache.

Ci sono due problemi principali: quali sono i punti dove fare prefetch nel codice. Posto che un fetch prende N cicli, allora
posso partire ponendo la prefetch distance a N cicli prima della load.
Se metto tutte le load all'inizio potrei fare cache trashing, ovvero la riempio di roba inutile.

Faccio un ragionamento analogo a quello del loop fusion: il prefetching è un bilancio tra anticipo e distanza nel tempo,
perché potrebbe accadere una eviction. La probabilità che qualcosa succeda non è nemmeno sotto il nostro controllo perché
in un sistema operativo moderno avvengono anche dei context switch.

Trasformazioni che rendono espliciti il numero di cicli, permettono di semplificare il calcolo del prefetching: loop unroll
e software pipelining.

### Software pipelining

Le iterazioni di un loop sono un distanziatore tra prefetch e load: sia $l$ la latenza della memoria e $s$ il percorso più
breve all'interno del loop, allora posso fare software pipelining.

Il loop viene diviso in _prologue_, _steady-state_ e _epilogue_:

- nel prologo si fanno X prefetch, dove X è sono le _iteration ahead_;

- nel steady state si fanno prefetch di i+X, ma si fanno N-X iterazioni;

- nell'epilogo non si fanno prefetch ma solo operazioni, quelle di coda che hanno i dati prefetched.
