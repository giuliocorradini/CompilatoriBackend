# Ottimizzazioni memoria

C'è un memory wall: DRAM e CPU sono basate su tecnologie diverse, quindi hanno performance differenti. Questo crea una
forbice di performance che cresce tanto più va avanti il tempo.

Per questo motivo nascono gerarchie di cache sempre più complesse, per nascondere questa latenza.

Le DRAM sono memorie off-chip, ovvero tipicamente si trova fuori da un circuito integrato. La motherboard contiene
tipicamente un socket per il chip della CPU e degli slot per alloggiare degli slot.
La comunicazione avviene tra canali che sono esterni alla CPU: uno dei principali motivi per cui la DRAM è lenta.

Inoltre a differenza della CPU (che è una rete logica stampata su un pezzo di silicio e pilotata da un singolo segnale di
clock), mentre la RAM si basa sulla capacità di un circuito (capacità come quello di un condensatore). Il fatto che ci sia
carica rappresenta un 1 logico, se la carica non è presente abbiamo uno 0.

La carica è volatile, perché nel tempo tendono a perdere la loro carica. La RAM viene quindi costantemente riscritta.

> La banda è la quantità di dati che posso trasportare in una singola transazione.

Nelle GPU le interfacce di memoria sono molto large (256/512b) per tenere occupati tutti i processori.

Siccome una transazione ha un costo, leggo una riga intera. Questo è giustificato da località spaziale dei dati che
manipolo. Se i dati che uso spesso vengono contenuti nella stessa riga, allora l'accesso sarà più veloce.

## SRAM

Ci sono altri tipi di tecnologie rispetto alla DRAM.

La SRAM ha la caratteristica di poter essere integrata dentro a un chip, e si utilizza principalmente per realizzare le
gerarchie di cache. Accedere a una SRAM è tipicamente comparabile alla velocità della CPU. Questo dipende anche dalla
distanza fisica tra cache e CPU.

La gerarchia di una cache e dei core è fisicamente distribuita, quindi è "vicina" a tutti i core. Colpisce che una porzione
significativa del SoC è occupata dalla cache. Parliamo di diversi MiB di capacità.

Cosa succederebbe se dovessimo implementare una quantità di memoria come la DRAM? Avremmo bisogno di un die con un'area
molto grande, nonché avremmo un consumo di memoria superiore. La DRAM è più piccola.

## Località

Se accedo a una locazione di memoria, probabilmente accederò anche a quelle contigue. La maggior parte dei programmi
spende tempo nei loop, che manipolano strutture dati tipicamente ospitate in memoria come celle adiacenti.

> Se questo è il modo più comune di usare gli array, progettiamo le cache in modo da portarmi dentro anche i dati
successivi. Questo prende nome di località spaziale.

Empiricamente si osserva questa tendenza di **località spaziale**.

> Se accedo a un elemento (indirizzo di memoria), spesso viene acceduto nuovamente prima o poi.

Questa è **località temporale**, e il prima o poi viene definito dalla struttura della cache.

Nel 1971 il memory gap inizia a essere rilevante, e gli ingegneri di IBM si pongono il problema. Pubblicano il paper
_Program Restructuing for Virtual Memory_. Siamo agli albori della cache.

Si analizza il trace degli accessi alla memoria. Ci sono zone senza località, e zone con accesso ripetuto allo stesso
indirizzo di memoria. Siamo in presenza del fenomeno di temporal locality. (Rette orizzontali)

In presenza di una retta diagonale, siamo in presenza di un loop che legge un array. Due pattern facilmente
identificabili e talmente ricorrenti che giustificano la nascita di un nuovo pezzo di hardware.

## Funzionamento

Il caching avviene anche tra disco e DRAM, ed è quello che fa la memoria paginata.

La cache si interpone tra CPU e DRAM e fa lo _snooping_ degli indirizzi. Ascolta l'indirizzo e copia in maniera preventiva
(oppure reattiva se faccio una miss), e mi porto dentro tutta una linea di cache.
Le linee sfruttano il principio di località spaziale, un indirizzo identifica n-word.
Il principio di località temporale è soddisfatto dalla cache stessa perché mantengo i dati altamente acceduti vicino a me.

### Tipi di miss

Le tre C:

- compulsory (cold cache miss), quando il dato non è presente nella cache;

- conflict, quando uso cache direct mapped o associative ma non completamente. Siccome la cache è una frazione della DRAM
molti indirizzi vanno rimappati sulla stessa linea e questo origina una conflict miss. C'è sempre un tradeoff, tanto che
le cache più utilizzate sono ibride, e vengono dette set associative.

- capacity, quando non c'è più spazio libero nella cache.

## Exploiting locality

Per sfruttare la località spaziale devo ammortizzare il costo di una miss, quindi mi porto dentro tutta una linea di cache.
Più è lungo un blocco più riesco a ridurre la miss rate per località spaziale.

La dimensione della cache è fissa, e linee più grandi potrebbero darmi anche più eviction dovute a una pollution. Questo è
dovuto alla competizione tra linee di cache, che dovrei poter sostituire continuamente.

Dentro a un compilatore riesco a ragionare sul capire come sfruttare al meglio la gerarchia di cache. I programmi sono tutti
diversi e a livello architetturale non si può risolvere il problema, quindi lo faccio nel compilatore che è il punto migliore.

## Obiettivi

Voglio **ridurre la latenza**, quindi fare in modo che la maggior parte degli accessi in memoria risultano come hits.

Per **massimizzare la larghezza di banda** devo sfruttare le letture burst della RAM, ovvero letture in maxi-transazioni
di linee continue nella DRAM. La granularità diaccesso alla DRAM è la cache.

## Oggetti

In un programma manipoliamo tre tipi di dato.

- Scalari: tipi di dato semplici, come variabili statiche globali, parametri a funzione.

- Strutture dati: la clasica struct del C che organizza i dati aggregandoli.

- Array.

### Scalari

Quando uso variabili scalari, spesso si traduce in registri. Quindi se lavoro a livello di registri, la cache non
interviene affatto. Quindi a livello di local variable non ho gran problemi.

In assenza di informazioni su come è fatto un vero register file, si può assumere che ci siano illimitati registri
virtuali quando si ottimizza il codice intermedio e quindi si usa `mem2reg`. Oppure si assume che non ci siano registri
quindi si usano una sfilza di L/S.

Nel backend poi si fa il mapping sui registri fisici. Fin tanto che ho registri li uso, ma dopo devo usare lo stack per
le variabili locali (principalmente lo usavamo per la gestione della calling convention, ma anche gli argomenti che non riesco
a gestire direttamente tramite registro vanno passati con lo stack).

Passi come `mem2reg` o il suo duale servono a ottimizzare questo. Register allocation (viene fatta nel backend) servono
proprio a gestire questo genere di problematiche.

Quindi anche nell'uso di scalari il problema c'è e i compilatori possono gestirlo.

### Strutture dati e puntatori

In una situazione agnostica, senza padding e allineamento alla machine word, un campo potrebbe finire tra due righe
di DRAM. Questo mi costringe a caricare due righe.

Dove non riesco ad allinearmi semplicemente cambiando posto ai campi, posso fare padding della struttura dati in modo
che siano allineati.

Per ottimizzare posso trasformare la struttura, spezzandola.
